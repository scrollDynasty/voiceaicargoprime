#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞–±–æ—Ç—ã Ollama
"""

import requests
import json
import sys

def test_ollama():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama"""
    ollama_url = "http://localhost:11434"
    model_name = "llama3.1:8b-instruct-q4_0"
    
    print("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ Ollama —Å–µ—Ä–≤–µ—Ä–∞...")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Å–µ—Ä–≤–µ—Ä–∞
    try:
        response = requests.get(f"{ollama_url}/api/tags")
        if response.status_code == 200:
            print("‚úÖ Ollama —Å–µ—Ä–≤–µ—Ä –¥–æ—Å—Ç—É–ø–µ–Ω")
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
            models = response.json().get('models', [])
            if models:
                print("\nüìã –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏:")
                for model in models:
                    print(f"   - {model['name']} ({model['size']} bytes)")
                    
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞—à—É –º–æ–¥–µ–ª—å
                model_names = [m['name'] for m in models]
                if model_name in model_names:
                    print(f"\n‚úÖ –ú–æ–¥–µ–ª—å {model_name} –Ω–∞–π–¥–µ–Ω–∞")
                else:
                    print(f"\n‚ö†Ô∏è  –ú–æ–¥–µ–ª—å {model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                    print(f"   –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: ollama pull {model_name}")
            else:
                print("\n‚ö†Ô∏è  –ù–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π")
                print(f"   –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: ollama pull {model_name}")
                
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {response.status_code}")
            
    except requests.exceptions.ConnectionError:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Ollama")
        print("   –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Å–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω: ollama serve")
        return False
        
    # –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
    print("\nü§ñ –û—Ç–ø—Ä–∞–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞...")
    try:
        test_prompt = "–ü—Ä–∏–≤–µ—Ç! –û—Ç–≤–µ—Ç—å –æ–¥–Ω–∏–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º."
        
        response = requests.post(
            f"{ollama_url}/api/generate",
            json={
                "model": model_name,
                "prompt": test_prompt,
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "num_predict": 50
                }
            }
        )
        
        if response.status_code == 200:
            result = response.json()
            print(f"‚úÖ –û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω: {result['response'].strip()}")
            print(f"   –í—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {result.get('total_duration', 0) / 1e9:.2f} —Å–µ–∫")
            return True
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {response.status_code}")
            print(f"   {response.text}")
            return False
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–æ–≤–æ–º –∑–∞–ø—Ä–æ—Å–µ: {e}")
        return False

if __name__ == "__main__":
    print("üöÄ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å Ollama\n")
    
    if test_ollama():
        print("\n‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")
        sys.exit(0)
    else:
        print("\n‚ùå –¢–µ—Å—Ç—ã –Ω–µ –ø—Ä–æ–π–¥–µ–Ω—ã")
        sys.exit(1)