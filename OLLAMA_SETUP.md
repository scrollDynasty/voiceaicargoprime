# Настройка Ollama для Voice AI Engine

## Установка Ollama

### Linux/Mac
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### Windows
Скачайте установщик с https://ollama.com/download

## Запуск Ollama сервера

```bash
ollama serve
```

Сервер запустится на порту 11434.

## Загрузка модели

Для этого проекта рекомендуется использовать модель LLaMA 3.1 8B (квантованная версия):

```bash
ollama pull llama3.1:8b-instruct-q4_0
```

### Альтернативные модели

Если у вас мало памяти, можно использовать более легкие модели:

```bash
# Очень легкая модель (требует ~3GB RAM)
ollama pull llama3.2:3b-instruct-q4_0

# Средняя модель (требует ~5GB RAM)
ollama pull llama3.1:8b-instruct-q4_0

# Большая модель (требует ~8GB RAM)
ollama pull llama3.1:8b-instruct-q8_0
```

## Проверка установки

1. Проверьте список загруженных моделей:
```bash
ollama list
```

2. Запустите тестовый скрипт:
```bash
python test_ollama.py
```

## Настройка в проекте

1. Скопируйте файл `.env.example` в `.env`:
```bash
cp .env.example .env
```

2. Отредактируйте параметры Ollama в `.env` файле:
```
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b-instruct-q4_0
```

## Решение проблем

### Ollama не запускается
- Проверьте, что порт 11434 не занят: `sudo lsof -i :11434`
- Перезапустите сервис: `sudo systemctl restart ollama` (если установлен как сервис)

### Модель не загружается
- Проверьте свободное место на диске (нужно минимум 5-10 GB)
- Проверьте интернет-соединение

### Медленная генерация ответов
- Используйте квантованные модели (q4_0, q5_0)
- Уменьшите параметр `num_predict` в коде
- Используйте GPU если доступен: `ollama serve --gpu`

## Производительность

Рекомендуемые системные требования:
- CPU: 4+ ядер
- RAM: 8GB минимум (16GB рекомендуется)
- Диск: 10GB свободного места
- GPU: Опционально, но значительно ускоряет работу